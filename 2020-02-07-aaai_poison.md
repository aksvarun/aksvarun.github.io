---
layout: post
title:  "Hidden Trigger Backdoor Attacks"
date:   2020-02-07 22:21:59 +00:00
image: /images/poison_teaser.png
categories: conference
author: "Akshayvarun Subramanya"
authors: " Aniruddha Saha, <strong>Akshayvarun Subramanya</strong>, Hamed Pirsiavash"
venue: "<strong>Oral</strong> presentation at 34th American Conference on Artificial Intelligence(AAAI)"
arxiv: https://arxiv.org/abs/1910.00033
---

We explore poisoning methods to introduce backdoors in neural networks. The trigger for the backdoor is revealed only during inference and hidden during the model training stage, which gives more capacity to the adversary.
<!-- code: https://github.com/UMBCvision/fooling_network_interpretation -->
