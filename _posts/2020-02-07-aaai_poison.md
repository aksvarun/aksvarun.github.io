---
layout: post
title:  "Hidden Trigger Backdoor Attacks"
date:   2020-02-07 22:21:59 +00:00
image: /images/poison_teaser.png
categories: conference
author: "Akshayvarun Subramanya"
authors: " Aniruddha Saha, <strong>Akshayvarun Subramanya</strong>, Hamed Pirsiavash"
venue: "<strong>Oral</strong> presentation at 34th American Conference on Artificial Intelligence(AAAI)"
arxiv: https://arxiv.org/abs/1910.00033
---

We explore poisoning methods to introduce backdoors in neural networks where the trigger remains hidden from the victim during training time. Hence, the poisoned examples cannot be identified upon manual inspection and the attacker can use the trigger to fool the model successfully at test time. 
<!-- code: https://github.com/UMBCvision/fooling_network_interpretation -->
